{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_3824419199136104758() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_3824419199136104758()\">Toggle show/hide</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파이썬 2와 파이썬 3 지원\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# 공통\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 일관된 출력을 위해 유사난수 초기화\n",
    "np.random.seed(42)\n",
    "\n",
    "# 맷플롯립 설정\n",
    "# 맷플롯립 설정\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# 한글출력\n",
    "matplotlib.rc('font', family='NanumBarunGothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 그림을 저장할 폴드\n",
    "PROJECT_ROOT_DIR = \"c:\\\\git\\\\hands_on_ml_link\"\n",
    "CHAPTER_ID = \"end_to_end_project\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = os.path.join(\"c:\\\\git\\\\hands_on_ml_link\",\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "    \n",
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 예시를 위해서 만든 것입니다. 사이킷런에는 train_test_split() 함수가 있습니다.\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "train_set, test_set = split_train_test(housing, 0.2)\n",
    "\n",
    "\n",
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def test_set_check(identifier, test_ratio, hash=hashlib.md5):\n",
    "    return bytearray(hash(np.int64(identifier)).digest())[-1] < 256 * test_ratio\n",
    "\n",
    "def test_set_check(identifier, test_ratio, hash=hashlib.md5):\n",
    "    return bytearray(hash(np.int64(identifier)).digest())[-1] < 256 * test_ratio\n",
    "\n",
    "housing_with_id = housing.reset_index()   # `index` 열이 추가된 데이터프레임이 반환됩니다.\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n",
    "\n",
    "housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "# 소득 카테고리 개수를 제한하기 위해 1.5로 나눕니다.\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "# 5 이상은 5로 레이블합니다.\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "\n",
    "def income_cat_proportions(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\": income_cat_proportions(housing),\n",
    "    \"Stratified\": income_cat_proportions(strat_test_set),\n",
    "    \"Random\": income_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100\n",
    "\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)\n",
    "\n",
    "housing = strat_train_set.copy()\n",
    "\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # 훈련 세트를 위해 레이블 삭제\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "housing_num = housing.drop('ocean_proximity', axis=1)\\\n",
    "    \n",
    "imputer.fit(housing_num)\n",
    "housing_num.median().values\n",
    "X = imputer.transform(housing_num)\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    "                          index = list(housing.index.values))\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns)\n",
    "\n",
    "housing_cat = housing['ocean_proximity']\n",
    "housing_cat_encoded, housing_categories = housing_cat.factorize()\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(categories='auto')\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import sparse\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self.encoding = encoding\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the CategoricalEncoder to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_feature]\n",
    "            The data to determine the categories of each feature.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
    "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n",
    "                        \"or 'ordinal', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                if not np.all(valid_mask):\n",
    "                    if self.handle_unknown == 'error':\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(np.sort(self.categories[i]))\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X using one-hot encoding.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to encode.\n",
    "        Returns\n",
    "        -------\n",
    "        X_out : sparse matrix or a 2-d array\n",
    "            Transformed input.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=np.int)\n",
    "        X_mask = np.ones_like(X, dtype=np.bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            valid_mask = np.in1d(X[:, i], self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    # Set the problematic rows to an acceptable value and\n",
    "                    # continue `The rows are marked `X_mask` and will be\n",
    "                    # removed later.\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    X[:, i][~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            return X_int.astype(self.dtype, copy=False)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        indices = np.cumsum(n_values)\n",
    "\n",
    "        column_indices = (X_int + indices[:-1]).ravel()[mask]\n",
    "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n",
    "                                n_features)[mask]\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n",
    "                                shape=(n_samples, indices[-1]),\n",
    "                                dtype=self.dtype).tocsr()\n",
    "        if self.encoding == 'onehot-dense':\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "cat_encoder = CategoricalEncoder()\n",
    "housing_cat_reshaped = housing_cat.values.reshape(-1, 1)\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat_reshaped)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder(categories='auto')\n",
    "housing_cat_reshaped = housing_cat.values.reshape(-1, 1)\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat_reshaped)\n",
    "cat_encoder = OneHotEncoder(categories='auto', sparse=False)\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat_reshaped)\n",
    "cat_encoder.categories_\n",
    "housing_cat = housing[['ocean_proximity']]\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder(categories='auto')\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "cat_encoder = OneHotEncoder(categories='auto', sparse=False)\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# 컬럼 인덱스\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "housing_extra_attribs = pd.DataFrame(\n",
    "    housing_extra_attribs, \n",
    "    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"])\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(categories='auto'), cat_attribs),\n",
    "    ])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# 사이킷런이 DataFrame을 바로 사용하지 못하므로\n",
    "# 수치형이나 범주형 컬럼을 선택하는 클래스를 만듭니다.\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)),\n",
    "        ('cat_encoder', CategoricalEncoder(encoding=\"onehot-dense\")),\n",
    "    ])\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num_pipeline\", num_pipeline, num_attribs),\n",
    "        (\"cat_encoder\", OneHotEncoder(categories='auto'), cat_attribs),\n",
    "    ])\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "\n",
    "from IPython.display import HTML\n",
    "import random\n",
    "\n",
    "def hide_toggle(for_next=False):\n",
    "    this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "    next_cell = this_cell + '.next()'\n",
    "\n",
    "    toggle_text = 'Toggle show/hide'  # text shown on toggle link\n",
    "    target_cell = this_cell  # target cell to control with toggle\n",
    "    js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "    if for_next:\n",
    "        target_cell = next_cell\n",
    "        toggle_text += ' next cell'\n",
    "        js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "    js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            function {f_name}() {{\n",
    "                {cell_selector}.find('div.input').toggle();\n",
    "            }}\n",
    "\n",
    "            {js_hide_current}\n",
    "        </script>\n",
    "\n",
    "        <a href=\"javascript:{f_name}()\">{toggle_text}</a>\n",
    "    \"\"\".format(\n",
    "        f_name=js_f_name,\n",
    "        cell_selector=target_cell,\n",
    "        js_hide_current=js_hide_current, \n",
    "        toggle_text=toggle_text\n",
    "    )\n",
    "\n",
    "    return HTML(html)\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. 머신러닝 프로젝트 처음부터 끝까지\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.6 모델 선택과 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 훈련 세트에서 훈련하고 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 선형 회귀 모델을 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()    # 선형 회귀 모델\n",
    "lin_reg.fit(housing_prepared, housing_labels)   # 훈련, 매개변수(Train data, Target values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 훈련 세트에 있는 몇 개 샘플에 대해 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17606</th>\n",
       "      <td>-121.89</td>\n",
       "      <td>37.29</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1568.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>710.0</td>\n",
       "      <td>339.0</td>\n",
       "      <td>2.7042</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18632</th>\n",
       "      <td>-121.93</td>\n",
       "      <td>37.05</td>\n",
       "      <td>14.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>6.4214</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14650</th>\n",
       "      <td>-117.20</td>\n",
       "      <td>32.77</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>471.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>2.8621</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>-119.61</td>\n",
       "      <td>36.31</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1847.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>1460.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>1.8839</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3555</th>\n",
       "      <td>-118.59</td>\n",
       "      <td>34.23</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6592.0</td>\n",
       "      <td>1525.0</td>\n",
       "      <td>4459.0</td>\n",
       "      <td>1463.0</td>\n",
       "      <td>3.0347</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "17606    -121.89     37.29                38.0       1568.0           351.0   \n",
       "18632    -121.93     37.05                14.0        679.0           108.0   \n",
       "14650    -117.20     32.77                31.0       1952.0           471.0   \n",
       "3230     -119.61     36.31                25.0       1847.0           371.0   \n",
       "3555     -118.59     34.23                17.0       6592.0          1525.0   \n",
       "\n",
       "       population  households  median_income ocean_proximity  \n",
       "17606       710.0       339.0         2.7042       <1H OCEAN  \n",
       "18632       306.0       113.0         6.4214       <1H OCEAN  \n",
       "14650       936.0       462.0         2.8621      NEAR OCEAN  \n",
       "3230       1460.0       353.0         1.8839          INLAND  \n",
       "3555       4459.0      1463.0         3.0347       <1H OCEAN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = housing.iloc[:5]\n",
    "some_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17606    286600.0\n",
       "18632    340600.0\n",
       "14650    196900.0\n",
       "3230      46300.0\n",
       "3555     254500.0\n",
       "Name: median_house_value, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_labels = housing_labels.iloc[:5]\n",
    "some_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측: [210644.60459286 317768.80697211 210956.43331178  59218.98886849\n",
      " 189747.55849879]\n"
     ]
    }
   ],
   "source": [
    "some_data_prepared = full_pipeline.transform(some_data) # 변환 파이프라인(데이터 전처리)\n",
    "\n",
    "print(\"예측:\", lin_reg.predict(some_data_prepared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실제 값과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"레이블:\", list(some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사이킷런의 mean_square_error 함수를 사용\n",
    "- 전체 훈련 세트에 대한 이 회귀 모델의 **RMSE**(평균 제곱근 오차<sup>Root Mean Square Error</sup>) 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68628.19819848923"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대부분 중간 주택 가격은 120,000에서 265,000 사이\n",
    "- 예측 오차가 68,628인 것은 훈련 데이터에 과소적합된 사례\n",
    "- 과소적합을 해결하는 주요 방법\n",
    "  - 더 강력한 모델을 선택\n",
    "  - 훈련 알고리즘에 더 좋은 특성을 주입\n",
    "  - 모델의 규제를 감소(이 모델은 규제 사용 X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 먼저 더 복잡한 모델인 DecisionTreeRegressor 모델을 시도\n",
    "- 이 모델은 강력하고 데이터에서 복잡한 비선형 관계를 찾을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 모델은 데이터에 과대적합됨\n",
    "- 훈련에 사용한 데이터를 예측에 그대로 사용하였기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 교차 검증을 사용한 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델을 평가하기 위해 사이킷런의 **교차 검증** 기능인 **K-겹 교차 검증**<sup>K-fold cross-validation</sup>을 수행\n",
    "- 훈련 세트를 **폴드**<sup>fold</sup>라 불리는 10개의 서브셋으로 무작위 분할 후, 10번 훈련하고 평가\n",
    "- 매번 다른 폴드 선택해 평가에 사용, 나머지는 훈련에 사용\n",
    "- 10개의 평가 점수가 담긴 배열이 결과가 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![k-fold_cross_validation](./images/k-fold_cross_validation.PNG)\n",
    "**<center>k-fold_cross_validation</center>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "점수: [70194.33680785 66855.16363941 72432.58244769 70758.73896782\n",
      " 71115.88230639 75585.14172901 70262.86139133 70273.6325285\n",
      " 75366.87952553 71231.65726027]\n",
      "평균: 71407.68766037929\n",
      "표준편차: 2439.4345041191004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores) # 사이킷런 교차검증의 score는 비용 함수가 아니라 효용 함수이므로 RMSE를 나타내기 위해 -를 붙임\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"점수:\", scores)\n",
    "    print(\"평균:\", scores.mean())\n",
    "    print(\"표준편차:\", scores.std())\n",
    "\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결정 트리 결과가 이전보다 나빠진 것을 볼 수 있다.\n",
    "- 교차 검증으로 모델의 성능 추정뿐 아니라 이 추정이 얼마나 정확한지(즉, 표준편차) 측정 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 선형 회귀 모델과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "점수: [66782.73843989 66960.118071   70347.95244419 74739.57052552\n",
      " 68031.13388938 71193.84183426 64969.63056405 68281.61137997\n",
      " 71552.91566558 67665.10082067]\n",
      "평균: 69052.46136345083\n",
      "표준편차: 2731.6740017983484\n"
     ]
    }
   ],
   "source": [
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확실히 결정 트리 모델이 과대적합되어 선형 회귀 모델보다 성능이 나쁘다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 마지막으로 RandomForestRegressor 모델 시도\n",
    "  - 특성을 무작위 선택하여 많은 결정 트리 만들고 그 예측을 평균낸다.\n",
    "  - 여러 다른 모델을 모아서 하나의 모델을 만드는 것을 **앙상블 학습**이라고 함\n",
    "  - 머신러닝 알고리즘의 성능을 극대화하는 방법 중 하나"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21933.31414779769"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "housing_predictions = forest_reg.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "점수: [51646.44545909 48940.60114882 53050.86323649 54408.98730149\n",
      " 50922.14870785 56482.50703987 51864.52025526 49760.85037653\n",
      " 55434.21627933 53326.10093303]\n",
      "평균: 52583.72407377466\n",
      "표준편차: 2298.353351147122\n"
     ]
    }
   ],
   "source": [
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
    "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이전 두 모델보다 훌륭함\n",
    "- 하지만, 훈련 세트에 대한 점수가 검증 세트에 대한 점수보다 낮으므로 이 모델도 과대적합되어 있음\n",
    "- 과대적합을 해결하는 방법\n",
    "  - 모델을 간단히 함\n",
    "  - 제한을 함(즉, 규제)\n",
    "  - 더 많은 훈련 데이터를 모음\n",
    "- 하나의 모델을 더 깊이 들어가기 전에, 다양한 모델을 시도해 가능성 있는 2~5개 정도의 모델을 선정하는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.7 모델 세부 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.1 그리드 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사이킷런의 GridSearchCV 사용\n",
    "- 탐색하고자 하는 하이퍼파라미터와 시도해볼 값을 지정하면 가능한 모든 하이퍼파라미터 조합에 대해 교차 검증을 사용해 평가함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다음 코드는 RandomForestRegressor에 대한 최적의 하이퍼파라미터 조합을 탐색함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid=[{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # 하이퍼파라미터 12(=3×4)개의 조합을 시도합니다.\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    # bootstrap은 False로 하고 6(=2×3)개의 조합을 시도합니다.\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "# 다섯 폴드에서 훈련하면 총 (12+6)*5=90번의 훈련이 일어납니다.\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', \n",
    "                           return_train_score=True, n_jobs=-1)\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최상의 파라미터 조합:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 8, 'n_estimators': 30}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 8과 30은 탐색 범위의 최댓값이기 때문에 더 큰 값으로 다시 검색해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 8, 'n_estimators': 80}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [\n",
    "    # 하이퍼파라미터 12(=3×4)개의 조합을 시도합니다.\n",
    "    {'n_estimators': [30, 70, 80, 90], 'max_features': [7, 8, 9]},\n",
    "    # bootstrap은 False로 하고 6(=2×3)개의 조합을 시도합니다.\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "# 다섯 폴드에서 훈련하면 총 (12+6)*5=90번의 훈련이 일어납니다.\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', \n",
    "                           return_train_score=True, n_jobs=-1)\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적의 추정기에 직접 접근할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=80, n_jobs=None, oob_score=False, random_state=42,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리드서치에서 테스트한 하이퍼파라미터 조합의 점수를 확인합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50026.483656917684 {'max_features': 7, 'n_estimators': 30}\n",
      "49522.69180975516 {'max_features': 7, 'n_estimators': 70}\n",
      "49488.890763687436 {'max_features': 7, 'n_estimators': 80}\n",
      "49365.35673615594 {'max_features': 7, 'n_estimators': 90}\n",
      "49682.25345942335 {'max_features': 8, 'n_estimators': 30}\n",
      "49190.483400224904 {'max_features': 8, 'n_estimators': 70}\n",
      "49186.35129771545 {'max_features': 8, 'n_estimators': 80}\n",
      "49205.83880612614 {'max_features': 8, 'n_estimators': 90}\n",
      "50346.58778956147 {'max_features': 9, 'n_estimators': 30}\n",
      "49550.15084319524 {'max_features': 9, 'n_estimators': 70}\n",
      "49512.47425439377 {'max_features': 9, 'n_estimators': 80}\n",
      "49454.15450873111 {'max_features': 9, 'n_estimators': 90}\n",
      "62895.088889905004 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n",
      "54658.14484390074 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n",
      "59470.399594730654 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n",
      "52725.01091081235 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n",
      "57490.612956065226 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n",
      "51009.51445842374 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 예에서는 max_features 하이퍼파라미터가 8, n_estimators 하이퍼파라미터가 80일 때 최적의 솔루션입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.2 랜덤 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그리드 탐색 방법은 비교적 적은 수의 조합을 탐구할 때 좋다.\n",
    "- 하지만 하이퍼파라미터 **탐색 공간**이 커지면 RandomizedSearchCV를 사용하는 편이 더 좋다.\n",
    "- RandomizedSearchCV는 각 반복마다 하이퍼파라미터에 임의의 수를 대입하여 지정한 횟수만큼 평가\n",
    "- 주요 장점\n",
    "  - 랜덤 탐색을 1,000회 반복하도록 실행하면 하이퍼파라미터마다 각기 다른 1,000개의 값을 탐색\n",
    "  - 단순히 반복 횟수를 조절하는 것만드로 하이퍼파라미터 탐색에 투입할 컴퓨팅 자원을 제어할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49150.657232934034 {'max_features': 7, 'n_estimators': 180}\n",
      "51389.85295710133 {'max_features': 5, 'n_estimators': 15}\n",
      "50796.12045980556 {'max_features': 3, 'n_estimators': 72}\n",
      "50835.09932039744 {'max_features': 5, 'n_estimators': 21}\n",
      "49280.90117886215 {'max_features': 7, 'n_estimators': 122}\n",
      "50774.86679035961 {'max_features': 3, 'n_estimators': 75}\n",
      "50682.75001237282 {'max_features': 3, 'n_estimators': 88}\n",
      "49608.94061293652 {'max_features': 5, 'n_estimators': 100}\n",
      "50473.57642831875 {'max_features': 3, 'n_estimators': 150}\n",
      "64429.763804893395 {'max_features': 5, 'n_estimators': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distribs = {\n",
    "        'n_estimators': randint(low=1, high=200),\n",
    "        'max_features': randint(low=1, high=8),\n",
    "    }\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n",
    "                                n_iter=10, cv=5, scoring='neg_mean_squared_error', \n",
    "                                random_state=42, n_jobs=-1)\n",
    "rnd_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "cvres = rnd_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤 탐색을 10회 반복하여 찾은 최적의 하이퍼파라미터는 max_features가 7, n_estimators가 180입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.3 앙상블 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 많은 결정 트리를 만들고 그 예측을 평균 내는 결정 트리의 앙상블인 랜덤 포레스트\n",
    "- 랜덤 포레스트가 결정 트리 하나보다 더 성능이 좋은 것처럼 모델의 그룹이 단일 모델보다 더 나은 성능을 발휘할 때가 많다.\n",
    "- 이 주제는 7장에서 자세히 살펴봄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.4 최상의 모델과 오차 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최상의 모델을 분석하면 문제에 대한 좋은 통찰을 얻는 경우가 많습니다.\n",
    "- RandomForestRegressor가 각 특성의 상대적은 중요도를 알려줍니다.\n",
    "- 이 정보를 바탕으로 덜 중요한 특성들을 제외할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.374854035523309, 'median_income'),\n",
       " (0.15818192974693593, 'INLAND'),\n",
       " (0.1105746932768499, 'pop_per_hhold'),\n",
       " (0.06996685111737724, 'longitude'),\n",
       " (0.06341900303356328, 'latitude'),\n",
       " (0.058144346936405765, 'bedrooms_per_room'),\n",
       " (0.049305211977929184, 'rooms_per_hhold'),\n",
       " (0.04367642913514129, 'housing_median_age'),\n",
       " (0.015113755864604974, 'population'),\n",
       " (0.014753420163486713, 'total_rooms'),\n",
       " (0.014692618908107455, 'total_bedrooms'),\n",
       " (0.014457888574140218, 'households'),\n",
       " (0.007408014014657358, '<1H OCEAN'),\n",
       " (0.0033086921914046484, 'NEAR OCEAN'),\n",
       " (0.0020681519829404043, 'NEAR BAY'),\n",
       " (7.495755314672751e-05, 'ISLAND')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "# cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat_encoder\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.5 테스트 세트로 시스템 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어느 정도 모델을 튜닝하여 만족할 만한 모델을 얻게 되면 테스트 세트에서 최종 모델을 평가함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47196.85609374548"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1) # 예측 변수\n",
    "y_test = strat_test_set[\"median_house_value\"].copy() # 레이블\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test) # 데이터 변환\n",
    "final_predictions = final_model.predict(X_test_prepared) # 예측\n",
    "\n",
    "# 평가\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.8 론칭, 모니터링, 그리고 시스템 유지 보수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 제품 시스템에 적용하기 위한 준비를 해야함\n",
    "  - 입력 데이터 소스를 우리 시스템에 연결하고 테스트 코드를 작성\n",
    "  - 일정 간격으로 시스템의 실시간 성능을 체크하고 성능이 떨어졌을 때 알람을 통지할 수 있는 모니터링 코드를 작성\n",
    "  - 시스템의 예측을 샘플링해서 평가함, 이런 과정에는 사람의 분석이 필요\n",
    "  - 시스템의 입력 데이터 품질 평가\n",
    "  - 마지막으로 새로운 데이터를 사용해 정기적으로 모델을 훈련, 가능하면 이 과정은 자동화함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.10 연습문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 서포트 벡터 머신 회귀(sklearn.svm.SVR)를 kernel=“linear”(하이퍼파라미터 C를 바꿔가며)나 kernel=“rbf”(하이퍼파라미터 C와 gamma를 바꿔가며) 등의 다양한 하이퍼파라미터 설정으로 시도해보세요. 지금은 이 하이퍼파라미터가 무엇을 의미하는지 너무 신경 쓰지 마세요. 최상의 SVR 모델은 무엇인가요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.9min\n",
      "C:\\Users\\link-Hwang\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 14.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
       "  gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid=[{'kernel': ['linear'], 'C': [10.0, 30.0, 100.0, 300.0, 1000.0, 3000.0, 10000.0, 30000.0]}, {'kernel': ['rbf'], 'C': [1.0, 3.0, 10.0, 30.0, 100.0, 300.0, 1000.0], 'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "param_grid = [\n",
    "        {'kernel': ['linear'], 'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]},\n",
    "        {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30., 100., 300., 1000.0],\n",
    "         'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},\n",
    "    ]\n",
    "\n",
    "svm_reg = SVR()\n",
    "grid_search = GridSearchCV(svm_reg, param_grid, cv=5, scoring='neg_mean_squared_error', \n",
    "                           verbose=2, n_jobs=-1)\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최상 모델의 (5-폴드 교차 검증으로 평가한) 점수는 다음과 같습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70363.90313964167"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_mse = grid_search.best_score_\n",
    "rmse = np.sqrt(-negative_mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최상의 하이퍼파라미터를 확인해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 30000.0, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2. GridSearchCV를 RandomizedSearchCV로 바꿔보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  3.5min\n",
      "C:\\Users\\link-Hwang\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 25.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "          estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
       "  gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          fit_params=None, iid='warn', n_iter=50, n_jobs=-1,\n",
       "          param_distributions={'kernel': ['linear', 'rbf'], 'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000015D179A2CC0>, 'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000015D179A2B38>},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring='neg_mean_squared_error',\n",
       "          verbose=2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import expon\n",
    "\n",
    "# 노트: kernel 매개변수가 \"linear\"일 때는 gamma가 무시됩니다.\n",
    "param_distribs = {\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'C': randint(low=10000, high=100000),\n",
    "        'gamma': expon(scale=1.0),\n",
    "    }\n",
    "\n",
    "svm_reg = SVR()\n",
    "rnd_search = RandomizedSearchCV(svm_reg, param_distributions=param_distribs,\n",
    "                                n_iter=50, cv=5, scoring='neg_mean_squared_error',\n",
    "                                verbose=2, n_jobs=-1, random_state=42)\n",
    "rnd_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최상 모델의 (5-폴드 교차 검증으로 평가한) 점수는 다음과 같습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55325.2999140845"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_mse = rnd_search.best_score_\n",
    "rmse = np.sqrt(-negative_mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최상의 하이퍼파라미터를 확인해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 99475, 'gamma': 0.37354658165762367, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 3. 가장 중요한 특성을 선택하는 변환기를 준비 파이프라인에 추가해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def indices_of_top_k(arr, k):\n",
    "    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n",
    "\n",
    "class TopFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_importances, k):\n",
    "        self.feature_importances = feature_importances\n",
    "        self.k = k\n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[:, self.feature_indices_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 특성 선택 클래스는 이미 어떤 식으로든 특성 중요도를 계산했다고 가정합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  7,  9, 12], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5 # 선택할 특성의 개수를 지정\n",
    "\n",
    "# 최상의 k개 특성의 인덱스를 확인\n",
    "top_k_feature_indices = indices_of_top_k(feature_importances, k)\n",
    "top_k_feature_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['longitude', 'latitude', 'median_income', 'pop_per_hhold',\n",
       "       'INLAND'], dtype='<U18')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(attributes)[top_k_feature_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최상의 k개 특성이 맞는지 다시 확인합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.374854035523309, 'median_income'),\n",
       " (0.15818192974693593, 'INLAND'),\n",
       " (0.1105746932768499, 'pop_per_hhold'),\n",
       " (0.06996685111737724, 'longitude'),\n",
       " (0.06341900303356328, 'latitude')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(feature_importances, attributes), reverse=True)[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이전에 정의한 준비 파이프라인과 특성 선택기를 추가한 새로운 파이프라인을 만듭니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "preparation_and_feature_selection_pipeline = Pipeline([\n",
    "    ('preparation', full_pipeline),\n",
    "    ('feature_selection', TopFeatureSelector(feature_importances, k))\n",
    "])\n",
    "housing_prepared_top_k_features = preparation_and_feature_selection_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "처음 3개 샘플의 특성을 확인해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.15604281,  0.77194962, -0.61493744, -0.08649871,  0.        ],\n",
       "       [-1.17602483,  0.6596948 ,  1.33645936, -0.03353391,  0.        ],\n",
       "       [ 1.18684903, -1.34218285, -0.5320456 , -0.09240499,  0.        ]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_prepared_top_k_features[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17606</th>\n",
       "      <td>-121.89</td>\n",
       "      <td>37.29</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1568.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>710.0</td>\n",
       "      <td>339.0</td>\n",
       "      <td>2.7042</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18632</th>\n",
       "      <td>-121.93</td>\n",
       "      <td>37.05</td>\n",
       "      <td>14.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>6.4214</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14650</th>\n",
       "      <td>-117.20</td>\n",
       "      <td>32.77</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>471.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>2.8621</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "17606    -121.89     37.29                38.0       1568.0           351.0   \n",
       "18632    -121.93     37.05                14.0        679.0           108.0   \n",
       "14650    -117.20     32.77                31.0       1952.0           471.0   \n",
       "\n",
       "       population  households  median_income ocean_proximity  \n",
       "17606       710.0       339.0         2.7042       <1H OCEAN  \n",
       "18632       306.0       113.0         6.4214       <1H OCEAN  \n",
       "14650       936.0       462.0         2.8621      NEAR OCEAN  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 4. 전체 데이터 준비 과정과 최종 예측을 하나의 파이프라인으로 만들어보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preparation', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('num_pipeline', Pipeline(memory=None,\n",
       "     steps=[('selector', DataFrameSelector(attribute_names=['longitude', 'latitude', 'housing_median_age', 't... gamma=0.37354658165762367, kernel='rbf', max_iter=-1, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_select_and_SVRpredict_pipeline = Pipeline([\n",
    "    ('preparation', full_pipeline),\n",
    "    ('feature_selection', TopFeatureSelector(feature_importances, 7)),\n",
    "    ('svm_reg', SVR(**rnd_search.best_params_))\n",
    "])\n",
    "prepare_select_and_SVRpredict_pipeline.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "몇 개의 샘플에 전체 파이프라인을 적용해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측:\t [188713.43230038 360472.01091988 171934.54128572  50446.19487884]\n",
      "레이블:\t\t [286600.0, 340600.0, 196900.0, 46300.0]\n"
     ]
    }
   ],
   "source": [
    "some_data = housing.iloc[:4]\n",
    "some_labels = housing_labels.iloc[:4]\n",
    "\n",
    "print(\"예측:\\t\", prepare_select_and_SVRpredict_pipeline.predict(some_data))\n",
    "print(\"레이블:\\t\\t\", list(some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에는 서포트 벡터 머신 회귀(SVR)을 사용, 아래는 RandomForestRegressor를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preparation', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('num_pipeline', Pipeline(memory=None,\n",
       "     steps=[('selector', DataFrameSelector(attribute_names=['longitude', 'latitude', 'housing_median_age', 't...tors=180, n_jobs=None, oob_score=False,\n",
       "           random_state=None, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_select_and_RFRpredict_pipeline = Pipeline([\n",
    "    ('preparation', full_pipeline),\n",
    "    ('feature_selection', TopFeatureSelector(feature_importances, 7)),\n",
    "    ('forest_reg', RandomForestRegressor(max_features=7, n_estimators=180))\n",
    "])\n",
    "prepare_select_and_RFRpredict_pipeline.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측:\t [258577.22222222 341968.34444444 209770.          55635.        ]\n",
      "레이블:\t\t [286600.0, 340600.0, 196900.0, 46300.0]\n"
     ]
    }
   ],
   "source": [
    "some_data = housing.iloc[:4]\n",
    "some_labels = housing_labels.iloc[:4]\n",
    "\n",
    "print(\"예측:\\t\", prepare_select_and_RFRpredict_pipeline.predict(some_data))\n",
    "print(\"레이블:\\t\\t\", list(some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 5. GridSearchCV를 사용해 준비 단계의 옵션을 자동으로 탐색해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[CV] feature_selection__k=1, preparation__num_pipeline__imputer__strategy=mean \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feature_selection__k=1, preparation__num_pipeline__imputer__strategy=mean, total=   5.2s\n",
      "[CV] feature_selection__k=1, preparation__num_pipeline__imputer__strategy=mean \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feature_selection__k=1, preparation__num_pipeline__imputer__strategy=mean, total=   5.2s\n",
      "[CV] feature_selection__k=1, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=1, preparation__num_pipeline__imputer__strategy=mean, total=   5.2s\n",
      "[CV] feature_selection__k=1, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=1, preparation__num_pipeline__imputer__strategy=mean, total=   5.2s\n",
      "[CV] feature_selection__k=1, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=1, preparation__num_pipeline__imputer__strategy=mean, total=   5.3s\n",
      "[CV] feature_selection__k=1, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=1, preparation__num_pipeline__imputer__strategy=median, total=   5.2s\n",
      "[CV] feature_selection__k=1, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=1, preparation__num_pipeline__imputer__strategy=median, total=   5.2s\n",
      "[CV] feature_selection__k=1, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=1, preparation__num_pipeline__imputer__strategy=median, total=   5.3s\n",
      "[CV] feature_selection__k=1, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=1, preparation__num_pipeline__imputer__strategy=median, total=   5.3s\n",
      "[CV] feature_selection__k=1, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=1, preparation__num_pipeline__imputer__strategy=median, total=   5.3s\n",
      "[CV] feature_selection__k=1, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=1, preparation__num_pipeline__imputer__strategy=most_frequent, total=   5.9s\n",
      "[CV] feature_selection__k=1, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=1, preparation__num_pipeline__imputer__strategy=most_frequent, total=   5.9s\n",
      "[CV] feature_selection__k=1, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=1, preparation__num_pipeline__imputer__strategy=most_frequent, total=   5.9s\n",
      "[CV] feature_selection__k=1, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=1, preparation__num_pipeline__imputer__strategy=most_frequent, total=   5.8s\n",
      "[CV] feature_selection__k=1, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=1, preparation__num_pipeline__imputer__strategy=most_frequent, total=   5.9s\n",
      "[CV] feature_selection__k=2, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=2, preparation__num_pipeline__imputer__strategy=mean, total=   5.5s\n",
      "[CV] feature_selection__k=2, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=2, preparation__num_pipeline__imputer__strategy=mean, total=   5.6s\n",
      "[CV] feature_selection__k=2, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=2, preparation__num_pipeline__imputer__strategy=mean, total=   5.6s\n",
      "[CV] feature_selection__k=2, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=2, preparation__num_pipeline__imputer__strategy=mean, total=   5.5s\n",
      "[CV] feature_selection__k=2, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=2, preparation__num_pipeline__imputer__strategy=mean, total=   5.5s\n",
      "[CV] feature_selection__k=2, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=2, preparation__num_pipeline__imputer__strategy=median, total=   5.5s\n",
      "[CV] feature_selection__k=2, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=2, preparation__num_pipeline__imputer__strategy=median, total=   5.6s\n",
      "[CV] feature_selection__k=2, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=2, preparation__num_pipeline__imputer__strategy=median, total=   5.5s\n",
      "[CV] feature_selection__k=2, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=2, preparation__num_pipeline__imputer__strategy=median, total=   5.5s\n",
      "[CV] feature_selection__k=2, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=2, preparation__num_pipeline__imputer__strategy=median, total=   5.5s\n",
      "[CV] feature_selection__k=2, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=2, preparation__num_pipeline__imputer__strategy=most_frequent, total=   6.1s\n",
      "[CV] feature_selection__k=2, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=2, preparation__num_pipeline__imputer__strategy=most_frequent, total=   6.4s\n",
      "[CV] feature_selection__k=2, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=2, preparation__num_pipeline__imputer__strategy=most_frequent, total=   6.1s\n",
      "[CV] feature_selection__k=2, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=2, preparation__num_pipeline__imputer__strategy=most_frequent, total=   6.2s\n",
      "[CV] feature_selection__k=2, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=2, preparation__num_pipeline__imputer__strategy=most_frequent, total=   6.1s\n",
      "[CV] feature_selection__k=3, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=3, preparation__num_pipeline__imputer__strategy=mean, total=   5.5s\n",
      "[CV] feature_selection__k=3, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=3, preparation__num_pipeline__imputer__strategy=mean, total=   5.7s\n",
      "[CV] feature_selection__k=3, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=3, preparation__num_pipeline__imputer__strategy=mean, total=   5.7s\n",
      "[CV] feature_selection__k=3, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=3, preparation__num_pipeline__imputer__strategy=mean, total=   5.7s\n",
      "[CV] feature_selection__k=3, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=3, preparation__num_pipeline__imputer__strategy=mean, total=   5.6s\n",
      "[CV] feature_selection__k=3, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=3, preparation__num_pipeline__imputer__strategy=median, total=   5.6s\n",
      "[CV] feature_selection__k=3, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=3, preparation__num_pipeline__imputer__strategy=median, total=   5.5s\n",
      "[CV] feature_selection__k=3, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=3, preparation__num_pipeline__imputer__strategy=median, total=   5.6s\n",
      "[CV] feature_selection__k=3, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=3, preparation__num_pipeline__imputer__strategy=median, total=   5.6s\n",
      "[CV] feature_selection__k=3, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=3, preparation__num_pipeline__imputer__strategy=median, total=   5.5s\n",
      "[CV] feature_selection__k=3, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=3, preparation__num_pipeline__imputer__strategy=most_frequent, total=   6.1s\n",
      "[CV] feature_selection__k=3, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=3, preparation__num_pipeline__imputer__strategy=most_frequent, total=   6.1s\n",
      "[CV] feature_selection__k=3, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=3, preparation__num_pipeline__imputer__strategy=most_frequent, total=   6.1s\n",
      "[CV] feature_selection__k=3, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=3, preparation__num_pipeline__imputer__strategy=most_frequent, total=   6.2s\n",
      "[CV] feature_selection__k=3, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=3, preparation__num_pipeline__imputer__strategy=most_frequent, total=   6.1s\n",
      "[CV] feature_selection__k=4, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=4, preparation__num_pipeline__imputer__strategy=mean, total=   5.9s\n",
      "[CV] feature_selection__k=4, preparation__num_pipeline__imputer__strategy=mean \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feature_selection__k=4, preparation__num_pipeline__imputer__strategy=mean, total=   5.9s\n",
      "[CV] feature_selection__k=4, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=4, preparation__num_pipeline__imputer__strategy=mean, total=   5.9s\n",
      "[CV] feature_selection__k=4, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=4, preparation__num_pipeline__imputer__strategy=mean, total=   5.9s\n",
      "[CV] feature_selection__k=4, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=4, preparation__num_pipeline__imputer__strategy=mean, total=   5.9s\n",
      "[CV] feature_selection__k=4, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=4, preparation__num_pipeline__imputer__strategy=median, total=   5.9s\n",
      "[CV] feature_selection__k=4, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=4, preparation__num_pipeline__imputer__strategy=median, total=   5.9s\n",
      "[CV] feature_selection__k=4, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=4, preparation__num_pipeline__imputer__strategy=median, total=   5.9s\n",
      "[CV] feature_selection__k=4, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=4, preparation__num_pipeline__imputer__strategy=median, total=   5.8s\n",
      "[CV] feature_selection__k=4, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=4, preparation__num_pipeline__imputer__strategy=median, total=   5.9s\n",
      "[CV] feature_selection__k=4, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=4, preparation__num_pipeline__imputer__strategy=most_frequent, total=   6.5s\n",
      "[CV] feature_selection__k=4, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=4, preparation__num_pipeline__imputer__strategy=most_frequent, total=   6.5s\n",
      "[CV] feature_selection__k=4, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=4, preparation__num_pipeline__imputer__strategy=most_frequent, total=   6.5s\n",
      "[CV] feature_selection__k=4, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=4, preparation__num_pipeline__imputer__strategy=most_frequent, total=   6.5s\n",
      "[CV] feature_selection__k=4, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=4, preparation__num_pipeline__imputer__strategy=most_frequent, total=   6.5s\n",
      "[CV] feature_selection__k=5, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=5, preparation__num_pipeline__imputer__strategy=mean, total=   6.1s\n",
      "[CV] feature_selection__k=5, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=5, preparation__num_pipeline__imputer__strategy=mean, total=   6.3s\n",
      "[CV] feature_selection__k=5, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=5, preparation__num_pipeline__imputer__strategy=mean, total=   6.2s\n",
      "[CV] feature_selection__k=5, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=5, preparation__num_pipeline__imputer__strategy=mean, total=   6.2s\n",
      "[CV] feature_selection__k=5, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=5, preparation__num_pipeline__imputer__strategy=mean, total=   6.1s\n",
      "[CV] feature_selection__k=5, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=5, preparation__num_pipeline__imputer__strategy=median, total=   6.2s\n",
      "[CV] feature_selection__k=5, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=5, preparation__num_pipeline__imputer__strategy=median, total=   6.2s\n",
      "[CV] feature_selection__k=5, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=5, preparation__num_pipeline__imputer__strategy=median, total=   6.2s\n",
      "[CV] feature_selection__k=5, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=5, preparation__num_pipeline__imputer__strategy=median, total=   6.2s\n",
      "[CV] feature_selection__k=5, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=5, preparation__num_pipeline__imputer__strategy=median, total=   6.1s\n",
      "[CV] feature_selection__k=5, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=5, preparation__num_pipeline__imputer__strategy=most_frequent, total=   6.8s\n",
      "[CV] feature_selection__k=5, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=5, preparation__num_pipeline__imputer__strategy=most_frequent, total=   6.9s\n",
      "[CV] feature_selection__k=5, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=5, preparation__num_pipeline__imputer__strategy=most_frequent, total=   6.8s\n",
      "[CV] feature_selection__k=5, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=5, preparation__num_pipeline__imputer__strategy=most_frequent, total=   6.7s\n",
      "[CV] feature_selection__k=5, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=5, preparation__num_pipeline__imputer__strategy=most_frequent, total=   6.7s\n",
      "[CV] feature_selection__k=6, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=6, preparation__num_pipeline__imputer__strategy=mean, total=   6.7s\n",
      "[CV] feature_selection__k=6, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=6, preparation__num_pipeline__imputer__strategy=mean, total=   6.5s\n",
      "[CV] feature_selection__k=6, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=6, preparation__num_pipeline__imputer__strategy=mean, total=   6.4s\n",
      "[CV] feature_selection__k=6, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=6, preparation__num_pipeline__imputer__strategy=mean, total=   6.5s\n",
      "[CV] feature_selection__k=6, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=6, preparation__num_pipeline__imputer__strategy=mean, total=   6.4s\n",
      "[CV] feature_selection__k=6, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=6, preparation__num_pipeline__imputer__strategy=median, total=   6.7s\n",
      "[CV] feature_selection__k=6, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=6, preparation__num_pipeline__imputer__strategy=median, total=   6.8s\n",
      "[CV] feature_selection__k=6, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=6, preparation__num_pipeline__imputer__strategy=median, total=   6.7s\n",
      "[CV] feature_selection__k=6, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=6, preparation__num_pipeline__imputer__strategy=median, total=   6.8s\n",
      "[CV] feature_selection__k=6, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=6, preparation__num_pipeline__imputer__strategy=median, total=   6.6s\n",
      "[CV] feature_selection__k=6, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=6, preparation__num_pipeline__imputer__strategy=most_frequent, total=   7.3s\n",
      "[CV] feature_selection__k=6, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=6, preparation__num_pipeline__imputer__strategy=most_frequent, total=   7.4s\n",
      "[CV] feature_selection__k=6, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=6, preparation__num_pipeline__imputer__strategy=most_frequent, total=   7.0s\n",
      "[CV] feature_selection__k=6, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=6, preparation__num_pipeline__imputer__strategy=most_frequent, total=   7.2s\n",
      "[CV] feature_selection__k=6, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=6, preparation__num_pipeline__imputer__strategy=most_frequent, total=   7.0s\n",
      "[CV] feature_selection__k=7, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=7, preparation__num_pipeline__imputer__strategy=mean, total=   6.7s\n",
      "[CV] feature_selection__k=7, preparation__num_pipeline__imputer__strategy=mean \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feature_selection__k=7, preparation__num_pipeline__imputer__strategy=mean, total=   6.7s\n",
      "[CV] feature_selection__k=7, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=7, preparation__num_pipeline__imputer__strategy=mean, total=   6.7s\n",
      "[CV] feature_selection__k=7, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=7, preparation__num_pipeline__imputer__strategy=mean, total=   7.2s\n",
      "[CV] feature_selection__k=7, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=7, preparation__num_pipeline__imputer__strategy=mean, total=   7.0s\n",
      "[CV] feature_selection__k=7, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=7, preparation__num_pipeline__imputer__strategy=median, total=   7.1s\n",
      "[CV] feature_selection__k=7, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=7, preparation__num_pipeline__imputer__strategy=median, total=   6.9s\n",
      "[CV] feature_selection__k=7, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=7, preparation__num_pipeline__imputer__strategy=median, total=   6.7s\n",
      "[CV] feature_selection__k=7, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=7, preparation__num_pipeline__imputer__strategy=median, total=   6.9s\n",
      "[CV] feature_selection__k=7, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=7, preparation__num_pipeline__imputer__strategy=median, total=   6.8s\n",
      "[CV] feature_selection__k=7, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=7, preparation__num_pipeline__imputer__strategy=most_frequent, total=   7.5s\n",
      "[CV] feature_selection__k=7, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=7, preparation__num_pipeline__imputer__strategy=most_frequent, total=   7.7s\n",
      "[CV] feature_selection__k=7, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=7, preparation__num_pipeline__imputer__strategy=most_frequent, total=   7.7s\n",
      "[CV] feature_selection__k=7, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=7, preparation__num_pipeline__imputer__strategy=most_frequent, total=   7.6s\n",
      "[CV] feature_selection__k=7, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=7, preparation__num_pipeline__imputer__strategy=most_frequent, total=   7.9s\n",
      "[CV] feature_selection__k=8, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=8, preparation__num_pipeline__imputer__strategy=mean, total=   9.0s\n",
      "[CV] feature_selection__k=8, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=8, preparation__num_pipeline__imputer__strategy=mean, total=   8.2s\n",
      "[CV] feature_selection__k=8, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=8, preparation__num_pipeline__imputer__strategy=mean, total=   9.1s\n",
      "[CV] feature_selection__k=8, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=8, preparation__num_pipeline__imputer__strategy=mean, total=   8.8s\n",
      "[CV] feature_selection__k=8, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=8, preparation__num_pipeline__imputer__strategy=mean, total=   8.7s\n",
      "[CV] feature_selection__k=8, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=8, preparation__num_pipeline__imputer__strategy=median, total=   8.1s\n",
      "[CV] feature_selection__k=8, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=8, preparation__num_pipeline__imputer__strategy=median, total=   7.9s\n",
      "[CV] feature_selection__k=8, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=8, preparation__num_pipeline__imputer__strategy=median, total=   8.2s\n",
      "[CV] feature_selection__k=8, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=8, preparation__num_pipeline__imputer__strategy=median, total=   7.9s\n",
      "[CV] feature_selection__k=8, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=8, preparation__num_pipeline__imputer__strategy=median, total=   9.1s\n",
      "[CV] feature_selection__k=8, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=8, preparation__num_pipeline__imputer__strategy=most_frequent, total=  10.0s\n",
      "[CV] feature_selection__k=8, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=8, preparation__num_pipeline__imputer__strategy=most_frequent, total=   9.2s\n",
      "[CV] feature_selection__k=8, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=8, preparation__num_pipeline__imputer__strategy=most_frequent, total=   9.5s\n",
      "[CV] feature_selection__k=8, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=8, preparation__num_pipeline__imputer__strategy=most_frequent, total=   8.5s\n",
      "[CV] feature_selection__k=8, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=8, preparation__num_pipeline__imputer__strategy=most_frequent, total=   9.4s\n",
      "[CV] feature_selection__k=9, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=9, preparation__num_pipeline__imputer__strategy=mean, total=  10.4s\n",
      "[CV] feature_selection__k=9, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=9, preparation__num_pipeline__imputer__strategy=mean, total=  10.1s\n",
      "[CV] feature_selection__k=9, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=9, preparation__num_pipeline__imputer__strategy=mean, total=  11.4s\n",
      "[CV] feature_selection__k=9, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=9, preparation__num_pipeline__imputer__strategy=mean, total=  10.6s\n",
      "[CV] feature_selection__k=9, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=9, preparation__num_pipeline__imputer__strategy=mean, total=  10.5s\n",
      "[CV] feature_selection__k=9, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=9, preparation__num_pipeline__imputer__strategy=median, total=  11.2s\n",
      "[CV] feature_selection__k=9, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=9, preparation__num_pipeline__imputer__strategy=median, total=  11.5s\n",
      "[CV] feature_selection__k=9, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=9, preparation__num_pipeline__imputer__strategy=median, total=  12.0s\n",
      "[CV] feature_selection__k=9, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=9, preparation__num_pipeline__imputer__strategy=median, total=  11.7s\n",
      "[CV] feature_selection__k=9, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=9, preparation__num_pipeline__imputer__strategy=median, total=  12.3s\n",
      "[CV] feature_selection__k=9, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=9, preparation__num_pipeline__imputer__strategy=most_frequent, total=  12.2s\n",
      "[CV] feature_selection__k=9, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=9, preparation__num_pipeline__imputer__strategy=most_frequent, total=  11.3s\n",
      "[CV] feature_selection__k=9, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=9, preparation__num_pipeline__imputer__strategy=most_frequent, total=  11.5s\n",
      "[CV] feature_selection__k=9, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=9, preparation__num_pipeline__imputer__strategy=most_frequent, total=  11.1s\n",
      "[CV] feature_selection__k=9, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=9, preparation__num_pipeline__imputer__strategy=most_frequent, total=  11.6s\n",
      "[CV] feature_selection__k=10, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=10, preparation__num_pipeline__imputer__strategy=mean, total=  13.8s\n",
      "[CV] feature_selection__k=10, preparation__num_pipeline__imputer__strategy=mean \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feature_selection__k=10, preparation__num_pipeline__imputer__strategy=mean, total=  13.9s\n",
      "[CV] feature_selection__k=10, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=10, preparation__num_pipeline__imputer__strategy=mean, total=  12.9s\n",
      "[CV] feature_selection__k=10, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=10, preparation__num_pipeline__imputer__strategy=mean, total=  12.0s\n",
      "[CV] feature_selection__k=10, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=10, preparation__num_pipeline__imputer__strategy=mean, total=  14.1s\n",
      "[CV] feature_selection__k=10, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=10, preparation__num_pipeline__imputer__strategy=median, total=  13.0s\n",
      "[CV] feature_selection__k=10, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=10, preparation__num_pipeline__imputer__strategy=median, total=  13.9s\n",
      "[CV] feature_selection__k=10, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=10, preparation__num_pipeline__imputer__strategy=median, total=  13.9s\n",
      "[CV] feature_selection__k=10, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=10, preparation__num_pipeline__imputer__strategy=median, total=  12.3s\n",
      "[CV] feature_selection__k=10, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=10, preparation__num_pipeline__imputer__strategy=median, total=  12.7s\n",
      "[CV] feature_selection__k=10, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=10, preparation__num_pipeline__imputer__strategy=most_frequent, total=  12.8s\n",
      "[CV] feature_selection__k=10, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=10, preparation__num_pipeline__imputer__strategy=most_frequent, total=  13.4s\n",
      "[CV] feature_selection__k=10, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=10, preparation__num_pipeline__imputer__strategy=most_frequent, total=  15.6s\n",
      "[CV] feature_selection__k=10, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=10, preparation__num_pipeline__imputer__strategy=most_frequent, total=  14.5s\n",
      "[CV] feature_selection__k=10, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=10, preparation__num_pipeline__imputer__strategy=most_frequent, total=  13.5s\n",
      "[CV] feature_selection__k=11, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=11, preparation__num_pipeline__imputer__strategy=mean, total=  13.1s\n",
      "[CV] feature_selection__k=11, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=11, preparation__num_pipeline__imputer__strategy=mean, total=  14.4s\n",
      "[CV] feature_selection__k=11, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=11, preparation__num_pipeline__imputer__strategy=mean, total=  15.5s\n",
      "[CV] feature_selection__k=11, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=11, preparation__num_pipeline__imputer__strategy=mean, total=  15.3s\n",
      "[CV] feature_selection__k=11, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=11, preparation__num_pipeline__imputer__strategy=mean, total=  15.3s\n",
      "[CV] feature_selection__k=11, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=11, preparation__num_pipeline__imputer__strategy=median, total=  16.3s\n",
      "[CV] feature_selection__k=11, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=11, preparation__num_pipeline__imputer__strategy=median, total=  13.8s\n",
      "[CV] feature_selection__k=11, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=11, preparation__num_pipeline__imputer__strategy=median, total=  17.2s\n",
      "[CV] feature_selection__k=11, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=11, preparation__num_pipeline__imputer__strategy=median, total=  14.7s\n",
      "[CV] feature_selection__k=11, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=11, preparation__num_pipeline__imputer__strategy=median, total=  14.0s\n",
      "[CV] feature_selection__k=11, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=11, preparation__num_pipeline__imputer__strategy=most_frequent, total=  16.0s\n",
      "[CV] feature_selection__k=11, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=11, preparation__num_pipeline__imputer__strategy=most_frequent, total=  16.1s\n",
      "[CV] feature_selection__k=11, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=11, preparation__num_pipeline__imputer__strategy=most_frequent, total=  16.1s\n",
      "[CV] feature_selection__k=11, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=11, preparation__num_pipeline__imputer__strategy=most_frequent, total=  15.5s\n",
      "[CV] feature_selection__k=11, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=11, preparation__num_pipeline__imputer__strategy=most_frequent, total=  16.1s\n",
      "[CV] feature_selection__k=12, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=12, preparation__num_pipeline__imputer__strategy=mean, total=  14.2s\n",
      "[CV] feature_selection__k=12, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=12, preparation__num_pipeline__imputer__strategy=mean, total=  15.1s\n",
      "[CV] feature_selection__k=12, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=12, preparation__num_pipeline__imputer__strategy=mean, total=  13.1s\n",
      "[CV] feature_selection__k=12, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=12, preparation__num_pipeline__imputer__strategy=mean, total=  14.9s\n",
      "[CV] feature_selection__k=12, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=12, preparation__num_pipeline__imputer__strategy=mean, total=  17.9s\n",
      "[CV] feature_selection__k=12, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=12, preparation__num_pipeline__imputer__strategy=median, total=  15.2s\n",
      "[CV] feature_selection__k=12, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=12, preparation__num_pipeline__imputer__strategy=median, total=  14.3s\n",
      "[CV] feature_selection__k=12, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=12, preparation__num_pipeline__imputer__strategy=median, total=  16.4s\n",
      "[CV] feature_selection__k=12, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=12, preparation__num_pipeline__imputer__strategy=median, total=  14.7s\n",
      "[CV] feature_selection__k=12, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=12, preparation__num_pipeline__imputer__strategy=median, total=  16.7s\n",
      "[CV] feature_selection__k=12, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=12, preparation__num_pipeline__imputer__strategy=most_frequent, total=  16.3s\n",
      "[CV] feature_selection__k=12, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=12, preparation__num_pipeline__imputer__strategy=most_frequent, total=  15.1s\n",
      "[CV] feature_selection__k=12, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=12, preparation__num_pipeline__imputer__strategy=most_frequent, total=  16.9s\n",
      "[CV] feature_selection__k=12, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=12, preparation__num_pipeline__imputer__strategy=most_frequent, total=  18.1s\n",
      "[CV] feature_selection__k=12, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=12, preparation__num_pipeline__imputer__strategy=most_frequent, total=  17.7s\n",
      "[CV] feature_selection__k=13, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=13, preparation__num_pipeline__imputer__strategy=mean, total=  16.8s\n",
      "[CV] feature_selection__k=13, preparation__num_pipeline__imputer__strategy=mean \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feature_selection__k=13, preparation__num_pipeline__imputer__strategy=mean, total=  17.7s\n",
      "[CV] feature_selection__k=13, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=13, preparation__num_pipeline__imputer__strategy=mean, total=  14.9s\n",
      "[CV] feature_selection__k=13, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=13, preparation__num_pipeline__imputer__strategy=mean, total=  15.7s\n",
      "[CV] feature_selection__k=13, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=13, preparation__num_pipeline__imputer__strategy=mean, total=  16.0s\n",
      "[CV] feature_selection__k=13, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=13, preparation__num_pipeline__imputer__strategy=median, total=  14.7s\n",
      "[CV] feature_selection__k=13, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=13, preparation__num_pipeline__imputer__strategy=median, total=  15.4s\n",
      "[CV] feature_selection__k=13, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=13, preparation__num_pipeline__imputer__strategy=median, total=  16.5s\n",
      "[CV] feature_selection__k=13, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=13, preparation__num_pipeline__imputer__strategy=median, total=  15.5s\n",
      "[CV] feature_selection__k=13, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=13, preparation__num_pipeline__imputer__strategy=median, total=  18.5s\n",
      "[CV] feature_selection__k=13, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=13, preparation__num_pipeline__imputer__strategy=most_frequent, total=  16.6s\n",
      "[CV] feature_selection__k=13, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=13, preparation__num_pipeline__imputer__strategy=most_frequent, total=  17.4s\n",
      "[CV] feature_selection__k=13, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=13, preparation__num_pipeline__imputer__strategy=most_frequent, total=  19.6s\n",
      "[CV] feature_selection__k=13, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=13, preparation__num_pipeline__imputer__strategy=most_frequent, total=  16.1s\n",
      "[CV] feature_selection__k=13, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=13, preparation__num_pipeline__imputer__strategy=most_frequent, total=  17.6s\n",
      "[CV] feature_selection__k=14, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=14, preparation__num_pipeline__imputer__strategy=mean, total=  17.1s\n",
      "[CV] feature_selection__k=14, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=14, preparation__num_pipeline__imputer__strategy=mean, total=  19.9s\n",
      "[CV] feature_selection__k=14, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=14, preparation__num_pipeline__imputer__strategy=mean, total=  16.2s\n",
      "[CV] feature_selection__k=14, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=14, preparation__num_pipeline__imputer__strategy=mean, total=  16.6s\n",
      "[CV] feature_selection__k=14, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=14, preparation__num_pipeline__imputer__strategy=mean, total=  18.4s\n",
      "[CV] feature_selection__k=14, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=14, preparation__num_pipeline__imputer__strategy=median, total=  20.0s\n",
      "[CV] feature_selection__k=14, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=14, preparation__num_pipeline__imputer__strategy=median, total=  18.2s\n",
      "[CV] feature_selection__k=14, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=14, preparation__num_pipeline__imputer__strategy=median, total=  18.4s\n",
      "[CV] feature_selection__k=14, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=14, preparation__num_pipeline__imputer__strategy=median, total=  16.0s\n",
      "[CV] feature_selection__k=14, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=14, preparation__num_pipeline__imputer__strategy=median, total=  16.6s\n",
      "[CV] feature_selection__k=14, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=14, preparation__num_pipeline__imputer__strategy=most_frequent, total=  16.2s\n",
      "[CV] feature_selection__k=14, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=14, preparation__num_pipeline__imputer__strategy=most_frequent, total=  17.2s\n",
      "[CV] feature_selection__k=14, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=14, preparation__num_pipeline__imputer__strategy=most_frequent, total=  20.1s\n",
      "[CV] feature_selection__k=14, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=14, preparation__num_pipeline__imputer__strategy=most_frequent, total=  16.0s\n",
      "[CV] feature_selection__k=14, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=14, preparation__num_pipeline__imputer__strategy=most_frequent, total=  18.0s\n",
      "[CV] feature_selection__k=15, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=15, preparation__num_pipeline__imputer__strategy=mean, total=  18.3s\n",
      "[CV] feature_selection__k=15, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=15, preparation__num_pipeline__imputer__strategy=mean, total=  18.8s\n",
      "[CV] feature_selection__k=15, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=15, preparation__num_pipeline__imputer__strategy=mean, total=1972.4min\n",
      "[CV] feature_selection__k=15, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=15, preparation__num_pipeline__imputer__strategy=mean, total=  17.1s\n",
      "[CV] feature_selection__k=15, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=15, preparation__num_pipeline__imputer__strategy=mean, total=  17.9s\n",
      "[CV] feature_selection__k=15, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=15, preparation__num_pipeline__imputer__strategy=median, total=  17.5s\n",
      "[CV] feature_selection__k=15, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=15, preparation__num_pipeline__imputer__strategy=median, total=  19.2s\n",
      "[CV] feature_selection__k=15, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=15, preparation__num_pipeline__imputer__strategy=median, total=  21.7s\n",
      "[CV] feature_selection__k=15, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=15, preparation__num_pipeline__imputer__strategy=median, total=  19.4s\n",
      "[CV] feature_selection__k=15, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=15, preparation__num_pipeline__imputer__strategy=median, total=  19.4s\n",
      "[CV] feature_selection__k=15, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=15, preparation__num_pipeline__imputer__strategy=most_frequent, total=  20.7s\n",
      "[CV] feature_selection__k=15, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=15, preparation__num_pipeline__imputer__strategy=most_frequent, total=  18.1s\n",
      "[CV] feature_selection__k=15, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=15, preparation__num_pipeline__imputer__strategy=most_frequent, total=  19.0s\n",
      "[CV] feature_selection__k=15, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=15, preparation__num_pipeline__imputer__strategy=most_frequent, total=  17.3s\n",
      "[CV] feature_selection__k=15, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=15, preparation__num_pipeline__imputer__strategy=most_frequent, total=  21.4s\n",
      "[CV] feature_selection__k=16, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=16, preparation__num_pipeline__imputer__strategy=mean, total=  16.9s\n",
      "[CV] feature_selection__k=16, preparation__num_pipeline__imputer__strategy=mean \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feature_selection__k=16, preparation__num_pipeline__imputer__strategy=mean, total=  18.1s\n",
      "[CV] feature_selection__k=16, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=16, preparation__num_pipeline__imputer__strategy=mean, total=  17.0s\n",
      "[CV] feature_selection__k=16, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=16, preparation__num_pipeline__imputer__strategy=mean, total=  19.4s\n",
      "[CV] feature_selection__k=16, preparation__num_pipeline__imputer__strategy=mean \n",
      "[CV]  feature_selection__k=16, preparation__num_pipeline__imputer__strategy=mean, total=  20.5s\n",
      "[CV] feature_selection__k=16, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=16, preparation__num_pipeline__imputer__strategy=median, total=  18.3s\n",
      "[CV] feature_selection__k=16, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=16, preparation__num_pipeline__imputer__strategy=median, total=  20.1s\n",
      "[CV] feature_selection__k=16, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=16, preparation__num_pipeline__imputer__strategy=median, total=  17.2s\n",
      "[CV] feature_selection__k=16, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=16, preparation__num_pipeline__imputer__strategy=median, total=  17.2s\n",
      "[CV] feature_selection__k=16, preparation__num_pipeline__imputer__strategy=median \n",
      "[CV]  feature_selection__k=16, preparation__num_pipeline__imputer__strategy=median, total=  20.1s\n",
      "[CV] feature_selection__k=16, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=16, preparation__num_pipeline__imputer__strategy=most_frequent, total=  17.5s\n",
      "[CV] feature_selection__k=16, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=16, preparation__num_pipeline__imputer__strategy=most_frequent, total=  17.6s\n",
      "[CV] feature_selection__k=16, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=16, preparation__num_pipeline__imputer__strategy=most_frequent, total=  20.4s\n",
      "[CV] feature_selection__k=16, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=16, preparation__num_pipeline__imputer__strategy=most_frequent, total=  18.7s\n",
      "[CV] feature_selection__k=16, preparation__num_pipeline__imputer__strategy=most_frequent \n",
      "[CV]  feature_selection__k=16, preparation__num_pipeline__imputer__strategy=most_frequent, total=  18.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed: 2029.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('preparation', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('num_pipeline', Pipeline(memory=None,\n",
       "     steps=[('selector', DataFrameSelector(attribute_names=['longitude', 'latitude', 'housing_median_age', 't... gamma=0.37354658165762367, kernel='rbf', max_iter=-1, shrinking=True,\n",
       "  tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=1,\n",
       "       param_grid=[{'preparation__num_pipeline__imputer__strategy': ['mean', 'median', 'most_frequent'], 'feature_selection__k': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=2)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [\n",
    "        {'preparation__num_pipeline__imputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "         'feature_selection__k': list(range(1, len(feature_importances) + 1))}\n",
    "]\n",
    "\n",
    "grid_search_prep = GridSearchCV(prepare_select_and_SVRpredict_pipeline, param_grid, cv=5,\n",
    "                                scoring='neg_mean_squared_error', verbose=2, n_jobs=1)\n",
    "grid_search_prep.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_selection__k': 10,\n",
       " 'preparation__num_pipeline__imputer__strategy': 'most_frequent'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_prep.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최상의 Imputer 정책은 most_frequent이고 16개 중 10개의 특성이 유용합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
